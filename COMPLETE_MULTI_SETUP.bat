@echo off
cls
echo.
echo  ‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
echo  ‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ñà‚ñà‚ïë      ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù
echo  ‚ñà‚ñà‚ïî‚ñà‚ñà‚ñà‚ñà‚ïî‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  
echo  ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ïö‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  
echo  ‚ñà‚ñà‚ïë ‚ïö‚ïê‚ïù ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
echo  ‚ïö‚ïê‚ïù     ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïù      ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù   ‚ïö‚ïê‚ïù    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
echo.
echo  üöÄ COMPLETE MULTI-STORAGE E-COMMERCE ANALYTICS PIPELINE
echo.
echo ========================================================================
echo   Advanced Data Pipeline with Multiple Storage Systems & ETL
echo ========================================================================
echo.
echo üéØ SYSTEM ARCHITECTURE:
echo.
echo   üìä DATA GENERATION
echo   ‚Ä¢ Realistic e-commerce events (customers, products, sessions)
echo   ‚Ä¢ 6 event types: page views, purchases, searches, interactions, engagement
echo   ‚Ä¢ Geographic and demographic data modeling
echo   ‚Ä¢ Data quality issues simulation (15%% error rate)
echo.
echo   üíæ MULTI-STORAGE SYSTEM
echo   ‚Ä¢ üìÅ Parquet Files: Columnar storage for analytics
echo   ‚Ä¢ ü¶Ü DuckDB: In-process analytical database
echo   ‚Ä¢ Random 50/50 distribution between storage types
echo   ‚Ä¢ Batch processing with configurable sizes
echo.
echo   üîÑ ETL PIPELINE
echo   ‚Ä¢ Extract from both Parquet and DuckDB
echo   ‚Ä¢ Transform with data cleaning and validation
echo   ‚Ä¢ Deduplicate records based on event_id
echo   ‚Ä¢ Load into Snowflake data warehouse
echo   ‚Ä¢ Comprehensive error handling and logging
echo.
echo   ‚ùÑÔ∏è  SNOWFLAKE DATA WAREHOUSE
echo   ‚Ä¢ Enterprise-grade data warehouse
echo   ‚Ä¢ Structured tables for all event types
echo   ‚Ä¢ Real-time analytics views
echo   ‚Ä¢ Automated processing tasks
echo   ‚Ä¢ Data quality monitoring
echo.
echo   üìä MULTI-DASHBOARD UI
echo   ‚Ä¢ üè† Main: Overall pipeline monitoring
echo   ‚Ä¢ üìÅ Parquet: File-based storage analytics
echo   ‚Ä¢ ü¶Ü DuckDB: Database storage monitoring
echo   ‚Ä¢ üîÑ ETL: Pipeline execution and logs
echo   ‚Ä¢ ‚ùÑÔ∏è  Snowflake: Data warehouse analytics
echo.
echo ========================================================================
echo   SETUP PHASES
echo ========================================================================
echo.
echo Phase 1: Snowflake Data Warehouse Setup
echo Phase 2: Multi-Storage Dashboard Launch
echo Phase 3: Data Generation and Processing
echo Phase 4: ETL Pipeline Integration
echo.
echo ‚ö†Ô∏è  PREREQUISITES:
echo   ‚Ä¢ Valid Snowflake account (configured in snowflake_config.json)
echo   ‚Ä¢ Python environment with virtual environment
echo   ‚Ä¢ Sufficient disk space for data files
echo.
echo Press any key to start the complete multi-storage setup...
pause >nul

echo.
echo ========================================================================
echo   PHASE 1: SNOWFLAKE DATA WAREHOUSE SETUP
echo ========================================================================
echo.
echo üèîÔ∏è Setting up Snowflake infrastructure...
echo   ‚Ä¢ Creating warehouse, database, and schemas
echo   ‚Ä¢ Setting up tables for all event types
echo   ‚Ä¢ Configuring analytics views and procedures
echo   ‚Ä¢ Setting up automated processing tasks
echo.

call setup_snowflake.bat

echo.
echo ‚úÖ Phase 1 Complete: Snowflake data warehouse ready!
echo.
echo ========================================================================
echo   PHASE 2: MULTI-STORAGE DASHBOARD LAUNCH
echo ========================================================================
echo.
echo üöÄ Launching multi-dashboard application...
echo   ‚Ä¢ Installing required dependencies (DuckDB, Pandas, PyArrow)
echo   ‚Ä¢ Initializing multi-storage manager
echo   ‚Ä¢ Starting Flask application with 5 dashboards
echo   ‚Ä¢ Setting up real-time WebSocket connections
echo.

start "Multi-Storage Dashboard" cmd /k "run_multi_dashboard.bat"

echo.
echo ‚úÖ Phase 2 Complete: Multi-dashboard application started!
echo.
echo Waiting for application to initialize...
timeout /t 15

echo.
echo ========================================================================
echo   PHASE 3: DATA GENERATION STATUS
echo ========================================================================
echo.
echo üìä Data generation will begin automatically when you:
echo   1. Open http://localhost:5000 (Main Dashboard)
echo   2. Click "Start Pipeline" button
echo   3. Monitor real-time data flow across storage systems
echo.
echo The system will:
echo   ‚Ä¢ Generate realistic e-commerce events
echo   ‚Ä¢ Randomly distribute data between Parquet and DuckDB (50/50)
echo   ‚Ä¢ Process data in configurable batches
echo   ‚Ä¢ Maintain data quality metrics
echo.
echo ========================================================================
echo   PHASE 4: ETL PIPELINE INTEGRATION
echo ========================================================================
echo.
echo üîÑ ETL Pipeline is ready for execution:
echo   ‚Ä¢ Manual execution via ETL Dashboard
echo   ‚Ä¢ Automatic deduplication of records
echo   ‚Ä¢ Data quality validation and reporting
echo   ‚Ä¢ Snowflake integration with structured tables
echo.
echo To run ETL:
echo   1. Navigate to http://localhost:5000/etl
echo   2. Click "Run ETL" button
echo   3. Monitor processing logs and statistics
echo   4. View results in Snowflake Dashboard
echo.
echo ========================================================================
echo   üéâ COMPLETE SETUP FINISHED!
echo ========================================================================
echo.
echo üåê DASHBOARD URLS:
echo   ‚Ä¢ Main Dashboard:      http://localhost:5000
echo   ‚Ä¢ Parquet Dashboard:   http://localhost:5000/parquet
echo   ‚Ä¢ DuckDB Dashboard:    http://localhost:5000/duckdb
echo   ‚Ä¢ ETL Dashboard:       http://localhost:5000/etl
echo   ‚Ä¢ Snowflake Dashboard: http://localhost:5000/snowflake
echo.
echo üìã NEXT STEPS:
echo   1. Open the Main Dashboard to start data generation
echo   2. Monitor data distribution across Parquet and DuckDB
echo   3. Run ETL pipeline to load data into Snowflake
echo   4. Explore analytics in the Snowflake Dashboard
echo   5. Use individual dashboards for detailed monitoring
echo.
echo üîß SYSTEM FEATURES:
echo   ‚Ä¢ Real-time event generation with quality simulation
echo   ‚Ä¢ Multi-storage architecture (Parquet + DuckDB)
echo   ‚Ä¢ ETL pipeline with deduplication and validation
echo   ‚Ä¢ Enterprise data warehouse (Snowflake)
echo   ‚Ä¢ 5 specialized monitoring dashboards
echo   ‚Ä¢ WebSocket real-time updates
echo   ‚Ä¢ Comprehensive logging and error handling
echo.
echo üìä DATA FLOW:
echo   Events ‚Üí [Parquet Files + DuckDB] ‚Üí ETL Pipeline ‚Üí Snowflake ‚Üí Analytics
echo.
echo Your advanced multi-storage e-commerce analytics pipeline is ready!
echo.
pause