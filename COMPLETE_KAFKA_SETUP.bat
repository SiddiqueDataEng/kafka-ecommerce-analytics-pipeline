@echo off
cls
echo.
echo  ‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ñà‚ïó
echo  ‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó    ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ïë
echo  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ñà‚ñà‚ñà‚ñà‚ïî‚ñà‚ñà‚ïë
echo  ‚ñà‚ñà‚ïî‚ïê‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë
echo  ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë ‚ïö‚ïê‚ïù ‚ñà‚ñà‚ïë
echo  ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù     ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù   ‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù     ‚ïö‚ïê‚ïù
echo.
echo  üöÄ COMPLETE KAFKA-ENABLED E-COMMERCE ANALYTICS PIPELINE
echo.
echo ========================================================================
echo   Advanced Real-time Data Pipeline with Apache Kafka Integration
echo ========================================================================
echo.
echo üéØ SYSTEM ARCHITECTURE:
echo.
echo   üì° KAFKA STREAMING LAYER
echo   ‚Ä¢ Real-time event ingestion and processing
echo   ‚Ä¢ Producer: Enhanced e-commerce event generation
echo   ‚Ä¢ Stream Processor: Data validation and enrichment
echo   ‚Ä¢ Consumer: Multi-storage distribution
echo   ‚Ä¢ Topics: raw_events, clean_events
echo.
echo   üìä DATA GENERATION
echo   ‚Ä¢ 6 realistic event types: PAGE_VIEW, PRODUCT_VIEW, ADD_TO_CART, PURCHASE, SEARCH, REVIEW
echo   ‚Ä¢ Comprehensive event metadata and business context
echo   ‚Ä¢ 15%% invalid events for data quality testing
echo   ‚Ä¢ Variable event rates based on business logic
echo.
echo   üíæ MULTI-STORAGE SYSTEM
echo   ‚Ä¢ üìÅ Parquet Files: Columnar storage for analytics
echo   ‚Ä¢ ü¶Ü DuckDB: In-process analytical database
echo   ‚Ä¢ Random 50/50 distribution between storage types
echo   ‚Ä¢ Batch processing with configurable sizes
echo.
echo   üîÑ ETL PIPELINE
echo   ‚Ä¢ Extract from Kafka topics, Parquet files, and DuckDB
echo   ‚Ä¢ Transform with data cleaning and validation
echo   ‚Ä¢ Deduplicate records based on event_id
echo   ‚Ä¢ Load into Snowflake data warehouse
echo   ‚Ä¢ Comprehensive error handling and logging
echo.
echo   ‚ùÑÔ∏è  SNOWFLAKE DATA WAREHOUSE
echo   ‚Ä¢ Enterprise-grade data warehouse
echo   ‚Ä¢ Structured tables for all event types
echo   ‚Ä¢ Real-time analytics views
echo   ‚Ä¢ Automated processing tasks
echo   ‚Ä¢ Data quality monitoring
echo.
echo   üìä MULTI-DASHBOARD UI
echo   ‚Ä¢ üè† Main: Overall pipeline monitoring with Kafka integration
echo   ‚Ä¢ üì° Kafka: Real-time event streaming with terminal view
echo   ‚Ä¢ üìÅ Parquet: File-based storage analytics
echo   ‚Ä¢ ü¶Ü DuckDB: Database storage monitoring
echo   ‚Ä¢ üîÑ ETL: Pipeline execution and logs
echo   ‚Ä¢ ‚ùÑÔ∏è  Snowflake: Data warehouse analytics
echo.
echo ========================================================================
echo   SETUP PHASES
echo ========================================================================
echo.
echo Phase 1: Kafka Dashboard Launch
echo Phase 2: Stream Processor Setup
echo Phase 3: Enhanced Producer Setup
echo Phase 4: Complete Pipeline Integration
echo.
echo ‚ö†Ô∏è  PREREQUISITES:
echo   ‚Ä¢ Apache Kafka running on localhost:9092
echo   ‚Ä¢ Valid Snowflake account (configured in snowflake_config.json)
echo   ‚Ä¢ Python environment with virtual environment
echo   ‚Ä¢ Sufficient disk space for data files
echo.
echo üìã KAFKA TOPICS REQUIRED:
echo   ‚Ä¢ raw_events (for incoming events)
echo   ‚Ä¢ clean_events (for processed events)
echo.
echo Press any key to start the complete Kafka-enabled setup...
pause >nul

echo.
echo ========================================================================
echo   PHASE 1: KAFKA DASHBOARD LAUNCH
echo ========================================================================
echo.
echo üöÄ Launching Kafka-enabled multi-dashboard application...
echo   ‚Ä¢ Real-time Kafka event streaming
echo   ‚Ä¢ Terminal-style event monitoring
echo   ‚Ä¢ WebSocket-based live updates
echo   ‚Ä¢ Integrated producer and consumer controls
echo.

start "Kafka Multi-Dashboard" cmd /k "run_kafka_dashboard.bat"

echo.
echo ‚úÖ Phase 1 Complete: Kafka dashboard application started!
echo.
echo Waiting for application to initialize...
timeout /t 15

echo.
echo ========================================================================
echo   PHASE 2: STREAM PROCESSOR SETUP
echo ========================================================================
echo.
echo üîÑ Setting up enhanced stream processor...
echo   ‚Ä¢ Comprehensive event validation
echo   ‚Ä¢ Business value scoring
echo   ‚Ä¢ Event categorization and enrichment
echo   ‚Ä¢ Real-time data quality monitoring
echo.

start "Stream Processor" cmd /k "run_stream_processor.bat"

echo.
echo ‚úÖ Phase 2 Complete: Stream processor ready!
echo.
echo Waiting for processor to initialize...
timeout /t 10

echo.
echo ========================================================================
echo   PHASE 3: ENHANCED PRODUCER SETUP
echo ========================================================================
echo.
echo üì° Setting up enhanced Kafka producer...
echo   ‚Ä¢ Realistic e-commerce event generation
echo   ‚Ä¢ 6 comprehensive event types
echo   ‚Ä¢ Variable event rates and business logic
echo   ‚Ä¢ Data quality simulation (15%% invalid events)
echo.

start "Kafka Producer" cmd /k "run_kafka_producer.bat"

echo.
echo ‚úÖ Phase 3 Complete: Enhanced producer ready!
echo.
echo Waiting for producer to start generating events...
timeout /t 10

echo.
echo ========================================================================
echo   PHASE 4: COMPLETE PIPELINE INTEGRATION
echo ========================================================================
echo.
echo üéØ Complete Kafka streaming pipeline is now operational:
echo.
echo üì° KAFKA FLOW:
echo   Producer ‚Üí raw_events ‚Üí Stream Processor ‚Üí clean_events ‚Üí Multi-Storage
echo.
echo üíæ STORAGE FLOW:
echo   Kafka Events ‚Üí [Parquet Files + DuckDB] ‚Üí ETL Pipeline ‚Üí Snowflake
echo.
echo üìä MONITORING:
echo   Real-time dashboards with live event streaming and terminal monitoring
echo.
echo ========================================================================
echo   üéâ COMPLETE KAFKA SETUP FINISHED!
echo ========================================================================
echo.
echo üåê DASHBOARD URLS:
echo   ‚Ä¢ Main Dashboard:      http://localhost:5004
echo   ‚Ä¢ Kafka Dashboard:     http://localhost:5004/kafka  ‚≠ê NEW!
echo   ‚Ä¢ Parquet Dashboard:   http://localhost:5004/parquet
echo   ‚Ä¢ DuckDB Dashboard:    http://localhost:5004/duckdb
echo   ‚Ä¢ ETL Dashboard:       http://localhost:5004/etl
echo   ‚Ä¢ Snowflake Dashboard: http://localhost:5004/snowflake
echo.
echo üìã NEXT STEPS:
echo   1. Open the Kafka Dashboard to see real-time event streaming
echo   2. Monitor the terminal view for live event processing
echo   3. Use dashboard controls to start/stop Kafka components
echo   4. Run ETL pipeline to load data into Snowflake
echo   5. Explore analytics in the Snowflake Dashboard
echo.
echo üîß KAFKA COMPONENTS RUNNING:
echo   ‚Ä¢ ‚úÖ Enhanced Producer: Generating realistic e-commerce events
echo   ‚Ä¢ ‚úÖ Stream Processor: Validating and enriching events
echo   ‚Ä¢ ‚úÖ Multi-Dashboard: Real-time monitoring and control
echo   ‚Ä¢ ‚úÖ Multi-Storage: Distributing events to Parquet and DuckDB
echo.
echo üìä DATA FLOW:
echo   Events ‚Üí Kafka ‚Üí Multi-Storage ‚Üí ETL ‚Üí Snowflake ‚Üí Analytics
echo.
echo üéØ FEATURES AVAILABLE:
echo   ‚Ä¢ Real-time event generation with business logic
echo   ‚Ä¢ Live event streaming with terminal monitoring
echo   ‚Ä¢ Data quality validation and scoring
echo   ‚Ä¢ Multi-storage architecture (Parquet + DuckDB)
echo   ‚Ä¢ ETL pipeline with deduplication
echo   ‚Ä¢ Enterprise data warehouse (Snowflake)
echo   ‚Ä¢ 6 specialized monitoring dashboards
echo   ‚Ä¢ WebSocket real-time updates
echo   ‚Ä¢ Comprehensive logging and error handling
echo.
echo Your complete Kafka-enabled e-commerce analytics pipeline is ready!
echo.
pause